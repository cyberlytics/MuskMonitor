{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Versuch die Sentiment-Analyse in das Stock Modell einfließen zu lassen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Vorverarbeitung der Sentimentdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "with open('tweets_with_classes.json', 'r') as file:\n",
    "    sentiment_data = json.load(file)\n",
    "\n",
    "sentiment_df = pd.DataFrame(sentiment_data)\n",
    "sentiment_df = sentiment_df.drop(columns=[\"title\"])\n",
    "sentiment_df['date'] = pd.to_datetime(sentiment_df['date'])\n",
    "sentiment_df['class'] = sentiment_df['class'].map({'Negative': -1, 'Neutral': 0, 'Positive': 1})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>description</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-06-13</td>\n",
       "      <td>@Damadeferroofic Will investigate</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-06-13</td>\n",
       "      <td>@WallStreetSilv This doesn’t make sense</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-06-13</td>\n",
       "      <td>@ScienceNews “Studies show …”</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-06-12</td>\n",
       "      <td>@JonErlichman Interesting</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-06-12</td>\n",
       "      <td>@DirtyTesLa Noted</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>2023-05-03</td>\n",
       "      <td>Cult / Culture</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>2023-05-03</td>\n",
       "      <td>@GaryMarcus @geoffreyhinton I’ve been saying t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>2023-05-03</td>\n",
       "      <td>@Timcast Go woke, go …\\r\\n\\r\\nIt’s been a whil...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>2023-05-03</td>\n",
       "      <td>@waitbutwhy Probably won’t even need to pay fo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>2023-05-02</td>\n",
       "      <td>@ggreenwald This will backfire</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>994 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          date                                        description  class\n",
       "0   2023-06-13                  @Damadeferroofic Will investigate      0\n",
       "1   2023-06-13            @WallStreetSilv This doesn’t make sense     -1\n",
       "2   2023-06-13                      @ScienceNews “Studies show …”      0\n",
       "3   2023-06-12                          @JonErlichman Interesting      1\n",
       "4   2023-06-12                                  @DirtyTesLa Noted      0\n",
       "..         ...                                                ...    ...\n",
       "989 2023-05-03                                     Cult / Culture      0\n",
       "990 2023-05-03  @GaryMarcus @geoffreyhinton I’ve been saying t...      0\n",
       "991 2023-05-03  @Timcast Go woke, go …\\r\\n\\r\\nIt’s been a whil...     -1\n",
       "992 2023-05-03  @waitbutwhy Probably won’t even need to pay fo...      0\n",
       "993 2023-05-02                     @ggreenwald This will backfire     -1\n",
       "\n",
       "[994 rows x 3 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Kombinieren der Stock-Daten und Sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          date                                        description  class\n",
      "0   2023-06-13                  @Damadeferroofic Will investigate      0\n",
      "1   2023-06-13            @WallStreetSilv This doesn’t make sense     -1\n",
      "2   2023-06-13                      @ScienceNews “Studies show …”      0\n",
      "3   2023-06-12                          @JonErlichman Interesting      1\n",
      "4   2023-06-12                                  @DirtyTesLa Noted      0\n",
      "..         ...                                                ...    ...\n",
      "988 2023-05-03  @AlexBerenson This is absurd. Shame on the CEO...     -1\n",
      "989 2023-05-03                                     Cult / Culture      0\n",
      "990 2023-05-03  @GaryMarcus @geoffreyhinton I’ve been saying t...      0\n",
      "991 2023-05-03  @Timcast Go woke, go …\\r\\n\\r\\nIt’s been a whil...     -1\n",
      "992 2023-05-03  @waitbutwhy Probably won’t even need to pay fo...      0\n",
      "\n",
      "[993 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "duplicates = sentiment_df[sentiment_df.duplicated(subset=['date'], keep=False)]\n",
    "print(duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('stock_data.json', 'r') as file:\n",
    "    stock_data = json.load(file)\n",
    "stock_data = pd.DataFrame(stock_data)\n",
    "stock_data['Datum'] = pd.to_datetime(stock_data['Datum'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Datum, open, high, low, close, volume]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "duplicates = stock_data[stock_data.duplicated(subset=['Datum'], keep=False)]\n",
    "print(duplicates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mehrere Tweets pro Tag vs. ein Stock Value pro Tag -> Wie geht man damit um?  \n",
    "Versuch 1: Mittelwertberechung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_df = sentiment_df.groupby('date').agg({'class': 'mean'}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = pd.merge(stock_data, sentiment_df, left_on='Datum', right_on='date', how='left', validate='one_to_one')\n",
    "\n",
    "# Fehlende Sentiments mit 0 auffüllen\n",
    "combined_data.fillna({\"class\": 0}, inplace=True)\n",
    "\n",
    "final_data = combined_data[['close', 'open', 'high', 'low', 'volume', 'class']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. LSTM-Modell Implementierung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelldefinition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class StockSentimentLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(StockSentimentLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])  # Nur die Ausgabe des letzten Zeitschritts\n",
    "        return out\n",
    "\n",
    "# Modell initialisieren\n",
    "input_size = 6  # 5 Stock-Daten + 1 Sentiment\n",
    "hidden_size = 64\n",
    "num_layers = 2\n",
    "output_size = 1\n",
    "\n",
    "model = StockSentimentLSTM(input_size, hidden_size, num_layers, output_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Franzi\\AppData\\Local\\Temp\\ipykernel_23128\\4186927211.py:23: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:277.)\n",
      "  X = torch.tensor(X, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Normalisiere die Daten\n",
    "scaler = MinMaxScaler()\n",
    "data_scaled = scaler.fit_transform(final_data)\n",
    "\n",
    "# Erstelle Eingabe- und Zielsequenzen\n",
    "def create_sequences(data, seq_length):\n",
    "    sequences = []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        seq = data[i:i+seq_length]\n",
    "        label = data[i+seq_length, 0]  # Vorhersage für 'close'\n",
    "        sequences.append((seq, label))\n",
    "    return sequences\n",
    "\n",
    "seq_length = 10\n",
    "sequences = create_sequences(data_scaled, seq_length)\n",
    "\n",
    "# Konvertiere in Tensoren\n",
    "X, y = zip(*sequences)\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 0.1904\n",
      "Epoch 2/50, Loss: 0.1719\n",
      "Epoch 3/50, Loss: 0.1543\n",
      "Epoch 4/50, Loss: 0.1374\n",
      "Epoch 5/50, Loss: 0.1213\n",
      "Epoch 6/50, Loss: 0.1059\n",
      "Epoch 7/50, Loss: 0.0914\n",
      "Epoch 8/50, Loss: 0.0781\n",
      "Epoch 9/50, Loss: 0.0664\n",
      "Epoch 10/50, Loss: 0.0572\n",
      "Epoch 11/50, Loss: 0.0514\n",
      "Epoch 12/50, Loss: 0.0501\n",
      "Epoch 13/50, Loss: 0.0524\n",
      "Epoch 14/50, Loss: 0.0548\n",
      "Epoch 15/50, Loss: 0.0538\n",
      "Epoch 16/50, Loss: 0.0491\n",
      "Epoch 17/50, Loss: 0.0421\n",
      "Epoch 18/50, Loss: 0.0349\n",
      "Epoch 19/50, Loss: 0.0286\n",
      "Epoch 20/50, Loss: 0.0237\n",
      "Epoch 21/50, Loss: 0.0200\n",
      "Epoch 22/50, Loss: 0.0171\n",
      "Epoch 23/50, Loss: 0.0144\n",
      "Epoch 24/50, Loss: 0.0115\n",
      "Epoch 25/50, Loss: 0.0084\n",
      "Epoch 26/50, Loss: 0.0053\n",
      "Epoch 27/50, Loss: 0.0029\n",
      "Epoch 28/50, Loss: 0.0019\n",
      "Epoch 29/50, Loss: 0.0028\n",
      "Epoch 30/50, Loss: 0.0043\n",
      "Epoch 31/50, Loss: 0.0046\n",
      "Epoch 32/50, Loss: 0.0037\n",
      "Epoch 33/50, Loss: 0.0031\n",
      "Epoch 34/50, Loss: 0.0034\n",
      "Epoch 35/50, Loss: 0.0043\n",
      "Epoch 36/50, Loss: 0.0049\n",
      "Epoch 37/50, Loss: 0.0049\n",
      "Epoch 38/50, Loss: 0.0042\n",
      "Epoch 39/50, Loss: 0.0033\n",
      "Epoch 40/50, Loss: 0.0026\n",
      "Epoch 41/50, Loss: 0.0023\n",
      "Epoch 42/50, Loss: 0.0023\n",
      "Epoch 43/50, Loss: 0.0022\n",
      "Epoch 44/50, Loss: 0.0018\n",
      "Epoch 45/50, Loss: 0.0014\n",
      "Epoch 46/50, Loss: 0.0011\n",
      "Epoch 47/50, Loss: 0.0010\n",
      "Epoch 48/50, Loss: 0.0011\n",
      "Epoch 49/50, Loss: 0.0012\n",
      "Epoch 50/50, Loss: 0.0013\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Loss und Optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training Loop\n",
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X)\n",
    "    loss = criterion(outputs.squeeze(), y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(X).squeeze().numpy()\n",
    "\n",
    "# Rücktransformation der Daten\n",
    "predictions = scaler.inverse_transform(np.column_stack((predictions, np.zeros((len(predictions), 5)))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.06671554e+02,  9.48800000e+00,  1.03313000e+01,\n",
       "         9.40330000e+00,  1.06541500e+07, -1.00000000e+00],\n",
       "       [ 2.98266963e+02,  9.48800000e+00,  1.03313000e+01,\n",
       "         9.40330000e+00,  1.06541500e+07, -1.00000000e+00],\n",
       "       [ 2.88528567e+02,  9.48800000e+00,  1.03313000e+01,\n",
       "         9.40330000e+00,  1.06541500e+07, -1.00000000e+00],\n",
       "       ...,\n",
       "       [ 2.52153711e+01,  9.48800000e+00,  1.03313000e+01,\n",
       "         9.40330000e+00,  1.06541500e+07, -1.00000000e+00],\n",
       "       [ 2.55276356e+01,  9.48800000e+00,  1.03313000e+01,\n",
       "         9.40330000e+00,  1.06541500e+07, -1.00000000e+00],\n",
       "       [ 2.56467708e+01,  9.48800000e+00,  1.03313000e+01,\n",
       "         9.40330000e+00,  1.06541500e+07, -1.00000000e+00]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
